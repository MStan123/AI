from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors import FlashrankRerank
from qdrant_client.models import VectorParams, Distance
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_core.documents import Document
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
from pathlib import Path
from diskcache import Cache
from dataclasses import dataclass
import hashlib
import qdrant_client
import json
import time
import math
import os
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from typing import List

cache = Cache("./llm_cache")

load_dotenv()

api_key = os.getenv("AZURE_OPENAI_API_KEY")
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")
deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT")
deployment_name_fallback = os.getenv("AZURE_OPENAI_DEPLOYMENT_FALLBACK")
api_version_fallback = os.getenv("AZURE_OPENAI_API_VERSION_FALLBACK")

# --------------------------------------------------------------
# 1. –ó–ê–ì–†–£–ó–ö–ê –ì–õ–ê–í–ù–û–ì–û –ò–ù–î–ï–ö–°–ê
# --------------------------------------------------------------
with open("index.json", 'r', encoding='utf-8') as f:
    index_data = json.load(f)

# --------------------------------------------------------------
# 2. EMBEDDINGS
# --------------------------------------------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
)
# --------------------------------------------------------------
# 3. –°–û–ó–î–ê–Å–ú –î–û–ö–£–ú–ï–ù–¢–´ –ò–ó –°–ê–ú–ú–ê–†–ò
# --------------------------------------------------------------
summary_documents = []
for i, chunk_info in enumerate(index_data["chunks"]):
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏ –∏ –≤–æ–ø—Ä–æ—Å—ã
    content = f"{chunk_info['summary']}\n" + "\n".join(chunk_info.get('questions', []))

    summary_documents.append(
        Document(
            page_content=content,
            metadata={
                "file": chunk_info["file"],
                "summary": chunk_info["summary"],
                "chunk_id": i
            }
        )
    )
# --------------------------------------------------------------
# 4. SETUP QDRANT
# --------------------------------------------------------------
client = qdrant_client.QdrantClient(
    url="http://localhost:6333",
    api_key=None,
)

collection_name = "summaries"

# –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
summary_store = QdrantVectorStore(
    client=client,
    collection_name=collection_name,
    embedding=embeddings,
)
if not client.collection_exists(collection_name):
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE),
    )
    summary_store.add_documents(summary_documents)

# Retriever –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
summary_retriever = summary_store.as_retriever(
    search_type="similarity",
    search_kwargs={'k': 20}  # –ë–µ—Ä—ë–º —Ç–æ–ø-3 —Ñ–∞–π–ª–∞
)

# --------------------------------------------------------------
# 5. FLASHRANK RERANKER
# --------------------------------------------------------------
FlashrankRerank.model_rebuild()
compressor = FlashrankRerank(
    model="ms-marco-MiniLM-L-12-v2",
    top_n=7
)

# --------------------------------------------------------------
# 6. AZURE OPENAI LLM
# --------------------------------------------------------------
llm = AzureChatOpenAI(
    azure_endpoint=endpoint,
    azure_deployment=deployment_name,
    api_version=api_version,
    api_key=api_key,
    temperature=1,
    max_retries=3,
    max_tokens=3000,
    timeout=30.0
)

fallback_llm = AzureChatOpenAI(
    azure_endpoint=endpoint,
    azure_deployment=deployment_name_fallback,
    api_version=api_version_fallback,
    api_key=api_key,
    temperature=1,
    max_retries=3,
    max_tokens=3000,
    timeout=30.0
)

# --------------------------------------------------------------
# 6. COST STATS
# --------------------------------------------------------------

@dataclass
class CostStats:
    llm_calls: int = 0
    cache_hits: int = 0
    spent_tokens: int = 0
    saved_tokens: int = 0


stats = CostStats()

def make_cache_key(query: str, context: str) -> str:
    raw = (query.strip().lower() + context).encode("utf-8")
    return hashlib.sha256(raw).hexdigest()

# --------------------------------------------------------------
# 7. RAG –§–£–ù–ö–¶–ò–Ø
# --------------------------------------------------------------
def answer_query(query: str):
    # –≠—Ç–∞–ø 1: retrieve —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ —Å–∞–º–º–∞—Ä–∏
    summary_docs = summary_retriever.invoke(query)
    selected_files = [doc.metadata["file"] for doc in summary_docs]

    # –≠—Ç–∞–ø 2: –∑–∞–≥—Ä—É–∑–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    chunks_dir = Path("chunks")
    detailed_docs = []
    for file_name in selected_files:
        file_path = chunks_dir / file_name
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                detailed_docs.append(
                    Document(
                        page_content=content,
                        metadata={
                            "source": file_name,
                            "type": "detailed_chunk"
                        }
                    )
                )
    if not detailed_docs:
        return "No relevant information found.", [], selected_files

    # –≠—Ç–∞–ø 3: Rerank —á–µ—Ä–µ–∑ FlashRank
    class SimpleRetriever(BaseRetriever):
        docs: list

        def _get_relevant_documents(self, query: str, **kwargs):
            return self.docs

    temp_retriever = SimpleRetriever(docs=detailed_docs)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=temp_retriever
    )
    reranked_docs = compression_retriever.invoke(query)

    # –≠—Ç–∞–ø 4: —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context = "\n\n".join(doc.page_content for doc in reranked_docs)

    # –≠—Ç–∞–ø 5: –∫—ç—à
    cache_key = make_cache_key(query, context)
    if cache_key in cache:
        cached = cache[cache_key]
        stats.cache_hits += 1
        stats.saved_tokens += cached["tokens"]
        print("From Cache:")
        return cached["response"], reranked_docs, selected_files

    # –≠—Ç–∞–ø 6: LLM –≤—ã–∑–æ–≤
    print("Request to Azure OpenAI")
    stats.llm_calls += 1
    messages = [
        {
            "role": "system",
            "content": (
                "You are the AI assistant of Birmarket support department.\n"
                "Answer ONLY using facts strictly from the provided context.\n"
                "If you not found any information say I don't know anything, call to our support.\n"
                "Do NOT invent information.\n"
                "Determine the user's language from their query.\n"
                "If the context is in another language, translate the answer to the user's language.\n"
                "Do NOT guess or invent information."
            )
        },
        {
            "role": "user",
            "content": f"Context:\n{context}\n\nQuestion: {query}"
        }
    ]

    try:
        response = llm.invoke(messages)
    except Exception:
        response = fallback_llm.invoke(messages)

    # –≠—Ç–∞–ø 7: –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤
    usage = response.response_metadata.get("usage", {})
    prompt_tokens = usage.get("prompt_tokens", 0)
    completion_tokens = usage.get("completion_tokens", 0)
    total_tokens = prompt_tokens + completion_tokens
    if total_tokens == 0:
        total_tokens = int(len((context + query + response.content).split()) * 1.3)

    stats.spent_tokens += total_tokens

    # –≠—Ç–∞–ø 8: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    cache.set(
        cache_key,
        {
            "response": response.content,
            "tokens": total_tokens
        },
        expire=60 * 60 * 24
    )

    return response.content, reranked_docs, selected_files


# --------------------------------------------------------------
# 8. METRICS FUNCTIONS
# --------------------------------------------------------------

def precision_at_k(retrieved_ids, relevant_ids, k):
    retrieved_k = retrieved_ids[:k]
    relevant = set(relevant_ids)
    hit_count = sum(1 for x in retrieved_k if x in relevant)
    return hit_count / k


def recall_at_k(retrieved_ids, relevant_ids, k):
    retrieved_k = retrieved_ids[:k]
    relevant = set(relevant_ids)
    hit_count = sum(1 for x in retrieved_k if x in relevant)
    return hit_count / len(relevant) if relevant else 0.0


def mrr(retrieved_ids, relevant_ids):
    relevant = set(relevant_ids)
    for idx, doc_id in enumerate(retrieved_ids, 1):
        if doc_id in relevant:
            return 1 / idx
    return 0.0


def dcg_at_k(retrieved_ids, relevant_ids, k):
    dcg = 0.0
    relevant = set(relevant_ids)
    for i, doc_id in enumerate(retrieved_ids[:k]):
        if doc_id in relevant:
            dcg += 1 / math.log2(i + 2)
    return dcg


def ndcg_at_k(retrieved_ids, relevant_ids, k):
    ideal_dcg = dcg_at_k(relevant_ids, relevant_ids, k)
    if ideal_dcg == 0:
        return 0.0
    return dcg_at_k(retrieved_ids, relevant_ids, k) / ideal_dcg


def retrieve_chunk_ids_two_stage(query, summary_ret, comp, k=20):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç chunk_id –∏–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    # –≠—Ç–∞–ø 1: –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–∞–º–º–∞—Ä–∏
    summary_docs = summary_ret.invoke(query)
    selected_files = [doc.metadata["file"] for doc in summary_docs]

    # –≠—Ç–∞–ø 2: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    chunks_dir = Path("chunks")
    detailed_docs = []

    for file_name in selected_files:
        file_path = chunks_dir / file_name
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # –ò–∑–≤–ª–µ–∫–∞–µ–º chunk_id –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, chunk_001.txt -> 1)
                chunk_id = int(file_name.replace('chunk_', '').replace('.txt', ''))
                detailed_docs.append(
                    Document(
                        page_content=content,
                        metadata={
                            "source": file_name,
                            "chunk_id": chunk_id,
                            "type": "detailed_chunk"
                        }
                    )
                )

    if not detailed_docs:
        return []

    # –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    temp_collection = f"temp_eval_{hash(query) % 10000}"
    collections = [c.name for c in client.get_collections().collections]
    if temp_collection in collections:
        client.delete_collection(temp_collection)

    client.create_collection(
        collection_name=temp_collection,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE)
    )

    temp_store = QdrantVectorStore(
        client=client,
        collection_name=temp_collection,
        embedding=embeddings,
    )
    temp_store.add_documents(detailed_docs)

    temp_retriever = temp_store.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 20}
    )

    # –≠—Ç–∞–ø 4: rerank
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=comp,
        base_retriever=temp_retriever
    )

    reranked_docs = compression_retriever.invoke(query)

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
    client.delete_collection(temp_collection)

    return [doc.metadata["chunk_id"] for doc in reranked_docs[:k]]


def evaluate_retriever(eval_dataset, summary_ret, comp, k_values=[5, 10, 20]):
    results = []
    for item in eval_dataset:
        query = item["query"]
        relevant = item["relevant_chunks"]
        retrieved_ids = retrieve_chunk_ids_two_stage(query, summary_ret, comp, max(k_values))
        metrics = {
            "query": query,
            "MRR": mrr(retrieved_ids, relevant)
        }
        for k in k_values:
            metrics[f"Precision@{k}"] = precision_at_k(retrieved_ids, relevant, k)
            metrics[f"Recall@{k}"] = recall_at_k(retrieved_ids, relevant, k)
            metrics[f"nDCG@{k}"] = ndcg_at_k(retrieved_ids, relevant, k)
        results.append(metrics)
    return results


def print_sources(docs):
    print("\n==== Retrieved & Reranked Chunks ====\n")
    for i, doc in enumerate(docs, 1):
        print(f"--- Chunk {i} ---")
        print("Metadata:", doc.metadata)
        print("Text:", doc.page_content[:300], "...\n")

def print_cost_report():
    total_queries = stats.llm_calls + stats.cache_hits
    potential_tokens = stats.spent_tokens + stats.saved_tokens

    print("\nüí∞ COST REPORT")
    print(f"Total queries:  {total_queries}")
    print(f"LLM calls:      {stats.llm_calls}")
    print(f"Cache hits:     {stats.cache_hits}")
    if total_queries > 0:
        hit_rate = (stats.cache_hits / total_queries) * 100
        print(f"Cache hit rate: {hit_rate:.1f}%")
    print(f"Spent tokens:   {stats.spent_tokens}")
    print(f"Saved tokens:   {stats.saved_tokens}")
    print(f"Potential tokens without cache: {potential_tokens}")
    if potential_tokens > 0:
        savings_percent = (stats.saved_tokens / potential_tokens) * 100
        print(f"Token savings:  {savings_percent:.1f}%")

    # –ü—Ä–∏–º–µ—Ä –¥–µ–Ω–µ–∂–Ω–æ–π —ç–∫–æ–Ω–æ–º–∏–∏ (Azure OpenAI GPT-4o –Ω–∞ –¥–µ–∫–∞–±—Ä—å 2025 ‚Äî –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–æ)
    # –ü–æ–¥—Å—Ç–∞–≤—å —Å–≤–æ–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã: input $ per 1k, output $ per 1k
    PRICE_INPUT = 0.00025 / 1000   # $0.25 / 1M input tokens ‚Äî –ø—Ä–∏–º–µ—Ä –¥–ª—è GPT-4o
    PRICE_OUTPUT = 0.000025 / 1000  # $0.025 / 1M output tokens ‚Äî –ø—Ä–∏–º–µ—Ä
    approx_cost_spent = stats.spent_tokens * ((PRICE_INPUT + PRICE_OUTPUT) / 2)
    approx_cost_saved = stats.saved_tokens * ((PRICE_INPUT + PRICE_OUTPUT) / 2)
    print(f"\nApproximate cost spent:  ${approx_cost_spent:.4f}")
    print(f"Approximate cost saved:  ${approx_cost_saved:.4f}")
    print(f"Total possible cost:     ${ (approx_cost_spent + approx_cost_saved):.4f}")

# --------------------------------------------------------------
# 9. RUN QUERY & METRICS
# --------------------------------------------------------------
query = "Salam. M…ôhsul sifari≈ü etmi≈ü…ôm, amma kuryer…ô z…ông vuranda telefonu g√∂t√ºrm√ºr. N…ô edim?"
start = time.perf_counter()

response, docs, selected_files = answer_query(query)

elapsed = time.perf_counter() - start

print("\n" + "=" * 50)
print("SELECTED FILES:", selected_files)
print("=" * 50)
print("\n========== ANSWER ==========")
print(response.content if hasattr(response, 'content') else response)
print(f"\nElapsed time: {elapsed:.2f} sec")

# ------------------ EVALUATION ------------------
eval_dataset = [
    {"query": "Birmarket n…ôdir?", "relevant_chunks": [1]},
    {"query": "Birmarket-d…ô hansƒ± √∂d…ôni≈ü √ºsullarƒ± m√∂vcuddur?", "relevant_chunks": [5, 6, 7]},
    {"query": "Kredit ≈ü…ôrtl…ôriniz n…ôdir?", "relevant_chunks": [11, 12, 13]},
    {"query": "Bonus √ºzr…ô kontekstd…ô olan b√ºt√ºn m…ôlumatlarƒ± ver", "relevant_chunks": [10, 43, 44, 45, 46, 47, 49]},
]

print("\n" + "="*60)
print("STARTING EVALUATION OF TWO-STAGE RETRIEVAL")
print("="*60)

'''eval_results = evaluate_retriever(eval_dataset, summary_retriever, compressor)

for r in eval_results:
    print("\nQuery:", r["query"])
    print("MRR:", round(r["MRR"], 4))
    for k in [5, 10, 20]:
        print(f"Precision@{k}: {r[f'Precision@{k}']:.4f}")
        print(f"Recall@{k}:    {r[f'Recall@{k}']:.4f}")
        print(f"nDCG@{k}:      {r[f'nDCG@{k}']:.4f}")

print("\n========== RERANKED DOCUMENTS (FlashRank) ==========")
print_sources(docs)
'''
print_cost_report()