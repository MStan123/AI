from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors import FlashrankRerank
from qdrant_client.models import VectorParams, Distance, Filter, FieldCondition, MatchValue
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_core.documents import Document
from langchain_openai import AzureChatOpenAI
from qdrant_client import QdrantClient
from dotenv import load_dotenv
from pathlib import Path
from dataclasses import dataclass
from metrics import ndcg_at_k, precision_at_k, recall_at_k, mrr
import json
import time
from langdetect import detect
import os
from langchain_core.retrievers import BaseRetriever
from sklearn.feature_extraction.text import TfidfVectorizer
from support_handoff import handoff

# ---------------------------
# ENV & CONFIG
# ---------------------------
load_dotenv()

api_key = os.getenv("AZURE_OPENAI_API_KEY")
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")
deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT")
deployment_name_fallback = os.getenv("AZURE_OPENAI_DEPLOYMENT_FALLBACK")
api_version_fallback = os.getenv("AZURE_OPENAI_API_VERSION_FALLBACK")

# --------------------------------------------------------------
# 1. –ó–ê–ì–†–£–ó–ö–ê –ì–õ–ê–í–ù–û–ì–û –ò–ù–î–ï–ö–°–ê
# --------------------------------------------------------------
with open("output1.json", 'r', encoding='utf-8') as f:
    index_data = json.load(f)

# --------------------------------------------------------------
# 2. EMBEDDINGS
# --------------------------------------------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
)

# --------------------------------------------------------------
# 3. –°–û–ó–î–ê–Å–ú –î–û–ö–£–ú–ï–ù–¢–´ –ò–ó –°–ê–ú–ú–ê–†–ò
# --------------------------------------------------------------
summary_documents = []
for i, chunk_info in enumerate(index_data["chunks"]):
    content = f"{chunk_info['summary']}\n" + "\n".join(chunk_info.get('questions', []))
    summary_documents.append(
        Document(
            page_content=content,
            metadata={
                "file": chunk_info["file"],
                "summary": chunk_info["summary"],
                "chunk_id": i + 1
            }
        )
    )

# ---------------------------
# BM25 –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
# ---------------------------
documents_texts = [doc.page_content for doc in summary_documents]
vectorizer = TfidfVectorizer()
X_sparse = vectorizer.fit_transform(documents_texts)

# --------------------------------------------------------------
# 4. SETUP QDRANT (–æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å —Å–∞–º–º–∞—Ä–∏)
# --------------------------------------------------------------
client = QdrantClient(url='http://localhost:6333', port=6333)
collection_name = "summaries"

summary_store = QdrantVectorStore(
    client=client,
    collection_name=collection_name,
    embedding=embeddings,
)

if not client.collection_exists(collection_name):
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE),
    )
    summary_store.add_documents(summary_documents)

# --------------------------------------------------------------
# SEMANTIC CACHE SETUP
# --------------------------------------------------------------
cache_collection_name = "rag_semantic_cache1"

if not client.collection_exists(cache_collection_name):
    client.create_collection(
        collection_name=cache_collection_name,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE),
    )

cache_vector_store = QdrantVectorStore(
    client=client,
    collection_name=cache_collection_name,
    embedding=embeddings,
)

# --------------------------------------------------------------
# SEMANTIC CACHE CLASS
# --------------------------------------------------------------
class RAGSemanticCache:
    def __init__(self, vector_store, threshold: float = 0.3):
        self.vector_store = vector_store
        self.threshold = threshold

    def retrieve_cached_response(self, query: str):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç Document —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏–ª–∏ None"""
        query_lang = detect(query)
        results = self.vector_store.similarity_search_with_score(
            query,
            k=1,
            score_threshold=self.threshold
        )
        if results:
            best, score = results[0]
            if best.metadata.get("language") == query_lang:
                return best

    def store_response(self, query: str, response: str, tokens: int):
        language = detect(query)
        doc = Document(
            page_content=query,  # —ç–º–±–µ–¥–¥–∏—Ç—Å—è –∏–º–µ–Ω–Ω–æ –∑–∞–ø—Ä–æ—Å
            metadata={
                "response": response,
                "tokens": tokens,
                "language": language
            }
        )
        self.vector_store.add_documents([doc])

semantic_cache = RAGSemanticCache(cache_vector_store, threshold=0.3)  # –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –ø–æ—Ä–æ–≥

# --------------------------------------------------------------
# 5. FLASHRANK RERANKER
# --------------------------------------------------------------
FlashrankRerank.model_rebuild()
compressor = FlashrankRerank(
    model="ms-marco-MiniLM-L-12-v2",
    top_n=25
)

# --------------------------------------------------------------
# 6. AZURE OPENAI LLM
# --------------------------------------------------------------
llm = AzureChatOpenAI(
    azure_endpoint=endpoint,
    azure_deployment=deployment_name,
    api_version=api_version,
    api_key=api_key,
    temperature=1,
    max_retries=3,
    max_tokens=3000,
    timeout=30.0
)

fallback_llm = AzureChatOpenAI(
    azure_endpoint=endpoint,
    azure_deployment=deployment_name_fallback,
    api_version=api_version_fallback,
    api_key=api_key,
    temperature=1,
    max_retries=3,
    max_tokens=3000,
    timeout=30.0
)

# --------------------------------------------------------------
# 7. COST STATS
# --------------------------------------------------------------
@dataclass
class CostStats:
    llm_calls: int = 0
    cache_hits: int = 0
    spent_tokens: int = 0
    saved_tokens: int = 0
    handoff_count: int = 0
    cached_responses: int = 0

stats = CostStats()

# --------------------------------------------------------------
# 8. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ BM25 + Qdrant
# --------------------------------------------------------------
def hybrid_summary_search(query, top_k=30, category=None):
    # --- BM25 ---
    query_vec = vectorizer.transform([query])
    bm25_scores = (X_sparse @ query_vec.T).toarray().flatten()
    bm25_top_idx = bm25_scores.argsort()[::-1][:top_k]
    bm25_docs = [summary_documents[i] for i in bm25_top_idx]

    # --- Qdrant semantic search ---
    query_embedding = embeddings.embed_query(query)
    filter_condition = None
    if category:
        filter_condition = Filter(
            must=[
                FieldCondition(
                    key="metadata.category",
                    match=MatchValue(value=category)
                )
            ]
        )

    search_result = client.query_points(
        collection_name=collection_name,
        query=query_embedding,
        limit=top_k,
        with_payload=True,
        query_filter=filter_condition,
    )

    qdrant_docs = []
    for hit in search_result.points:
        file_name = hit.payload.get("file")
        for doc in summary_documents:
            if doc.metadata["file"] == file_name:
                qdrant_docs.append(doc)
                break

    # --- –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª—É ---
    combined_docs = bm25_docs + qdrant_docs
    seen = set()
    final_docs = []
    for doc in combined_docs:
        file_key = doc.metadata["file"]
        if file_key not in seen:
            final_docs.append(doc)
            seen.add(file_key)

    return final_docs[:top_k]

# --------------------------------------------------------------
# HUMAN HANDOFF
# --------------------------------------------------------------
def needs_human_handoff(response: str, context: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–∞ –ª–∏ –ø–µ—Ä–µ–¥–∞—á–∞ —á–µ–ª–æ–≤–µ–∫—É"""
    no_info_phrases = [
        "don't have exact information",
        "–Ω–µ—Ç —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
        "–Ω–µ –∏–º–µ—é —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
        "d…ôqiq m…ôlumat yoxdur",
        "recommend contacting",
        "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é —Å–≤—è–∑–∞—Ç—å—Å—è",
        "unfortunately",
        "–∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é",
        "t…ô…ôss√ºf ki",
        "not found",
        "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    ]

    response_lower = response.lower()

    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
    if len(context.strip()) < 50:
        return True

    # –ï—Å–ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ –µ—Å—Ç—å —Ñ—Ä–∞–∑—ã "–Ω–µ –∑–Ω–∞—é"
    return any(phrase.lower() in response_lower for phrase in no_info_phrases)

# --------------------------------------------------------------
# 9. RAG –§–£–ù–ö–¶–ò–Ø
# --------------------------------------------------------------
def answer_query(query: str):
    # –≠—Ç–∞–ø 1: –≥–∏–±—Ä–∏–¥–Ω—ã–π retrieval
    summary_docs = hybrid_summary_search(query, top_k=30)
    selected_files = [doc.metadata["file"] for doc in summary_docs]

    # –≠—Ç–∞–ø 2: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    chunks_dir = Path("chunks")
    detailed_docs = []
    for file_name in selected_files:
        file_path = chunks_dir / file_name
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                detailed_docs.append(
                    Document(
                        page_content=content,
                        metadata={
                            "source": file_name,
                            "type": "detailed_chunk"
                        }
                    )
                )

    if not detailed_docs:
        return "No relevant information found.", [], selected_files

    # –≠—Ç–∞–ø 3: Rerank —á–µ—Ä–µ–∑ FlashRank
    class SimpleRetriever(BaseRetriever):
        docs: list
        def _get_relevant_documents(self, query: str, **kwargs):
            return self.docs

    temp_retriever = SimpleRetriever(docs=detailed_docs)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=temp_retriever
    )
    reranked_docs = compression_retriever.invoke(query)

    # –≠—Ç–∞–ø 4: —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context = "\n\n".join(doc.page_content for doc in reranked_docs)

    # --------------------------------------------------
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫—ç—à–∞
    # --------------------------------------------------
    cached_doc = semantic_cache.retrieve_cached_response(query)
    if cached_doc:
        stats.cache_hits += 1
        cached_tokens = cached_doc.metadata.get("tokens", 0)
        stats.saved_tokens += cached_tokens
        print("From Semantic Cache")
        return cached_doc.metadata["response"], reranked_docs, selected_files

    # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∫—ç—à–µ ‚Äî –∏–¥—ë–º –≤ LLM
    print("Request to Azure OpenAI")
    stats.llm_calls += 1

    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly and professional AI assistant for customer support .\n\n"

                "MAIN RULES:\n"
                "1. Answer ONLY based on the provided context if the question is about products, delivery, payment, returns, bonuses, customs, the app, website, or related services.\n"
                "2. If the context does not contain the information ‚Äî honestly say: 'Unfortunately, I don't have exact information on this question. I recommend contacting our support team at ... or via the chat in the app.'\n"
                "3. NEVER invent facts about Birmarket, prices, terms, product availability and etc.\n\n"

                "ALLOWED GENERAL QUESTIONS (answer them naturally and kindly):\n"
                "- Greetings (hi, salam, hello, good day, etc.)\n"
                "- Questions about the language of communication ('Can I ask in Russian?', 'In English?', 'Az…ôrbaycanca?')\n"
                "- Thanks ('thank you', 't…ô≈ü…ôkk√ºr')\n"
                "- Questions like 'How are you?', 'What can you do?'\n"
                "- Requests to repeat or clarify\n\n"

                "IMPORTANT:\n"
                "- Determine the user's language from their message and reply in the same language.\n"
                "- The user's question may be in Russian, Azerbaijani or other languages.\n"
                "- You MUST understand the Azerbaijani context and answer in the language of the user's question.\n"
                "- ALWAYS translate relevant facts from the Azerbaijani context into the user's language accurately.\n"
                "- If there is relevant information in the context (even if it's in Azerbaijani), use it and translate the answer.\n"
                "- Be polite, positive, and concise.\n"
                "- You are NOT a universal GPT. If the question clearly goes beyond support (politics, weather, programming, personal advice, etc.) ‚Äî politely redirect: 'Sorry, I specialize only in helping with purchases and Birmarket services. For other questions, I recommend contacting other services.'\n"
                "- Do not discuss your model, training, xAI, Grok, etc."
            )
        },
        {
            "role": "user",
            "content": f"Context:\n{context}\n\nQuestion: {query}"
        }
    ]

    try:
        response = llm.invoke(messages)
    except Exception:
        response = fallback_llm.invoke(messages)

    # –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤
    usage = response.response_metadata.get("usage", {})
    prompt_tokens = usage.get("prompt_tokens", 0)
    completion_tokens = usage.get("completion_tokens", 0)
    total_tokens = prompt_tokens + completion_tokens
    if total_tokens == 0:
        total_tokens = int(len((context + query + response.content).split()) * 1.3)

    stats.spent_tokens += total_tokens

    # --------------------------------------------------
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à
    # --------------------------------------------------
    if needs_human_handoff(response.content, context):
        # –°–æ–∑–¥–∞—ë–º —Å–µ—Å—Å–∏—é –ø–æ–¥–¥–µ—Ä–∂–∫–∏
        session_id = handoff.create_session(
            query=query,
            context=context,
            user_id=None,  # –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π user_id –µ—Å–ª–∏ –µ—Å—Ç—å
            user_phone=None,
            user_name=None
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞
        response_lang = detect(response.content) if response.content else 'en'

        # URL –¥–ª—è —á–∞—Ç–∞
        chat_url = f"http://localhost:8001/chat?session={session_id}"

        handoff_messages = {
            'ru': (
                f"\n\nüìû –°–æ–µ–¥–∏–Ω—è—é –≤–∞—Å —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º –ø–æ–¥–¥–µ—Ä–∂–∫–∏...\n"
                f"üé´ –ù–æ–º–µ—Ä –æ–±—Ä–∞—â–µ–Ω–∏—è: #{session_id[:8].upper()}\n"
                f"‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è: ~2-3 –º–∏–Ω—É—Ç—ã\n\n"
                f"–ß–∞—Ç –æ—Ç–∫—Ä–æ–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–ª–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ:\n"
                f"{chat_url}"
            ),
            'az': (
                f"\n\nüìû Sizi d…ôst…ôk m√ºt…ôx…ôssisi il…ô …ôlaq…ôl…ôndirir…ôm...\n"
                f"üé´ M√ºraci…ôt n√∂mr…ôsi: #{session_id[:8].upper()}\n"
                f"‚è±Ô∏è Orta g√∂zl…ôm…ô vaxtƒ±: ~2-3 d…ôqiq…ô\n\n"
                f"√áat avtomatik a√ßƒ±lacaq v…ô ya ke√ßid:\n"
                f"{chat_url}"
            ),
            'en': (
                f"\n\nüìû Connecting you with a support specialist...\n"
                f"üé´ Ticket number: #{session_id[:8].upper()}\n"
                f"‚è±Ô∏è Average wait time: ~2-3 minutes\n\n"
                f"Chat will open automatically or follow the link:\n"
                f"{chat_url}"
            )
        }

        response.content += handoff_messages.get(response_lang, handoff_messages['en'])

        stats.handoff_count += 1
        print(f"‚ö†Ô∏è HUMAN HANDOFF TRIGGERED - Session: {session_id}")
        print("‚ö†Ô∏è Response NOT cached (requires handoff)")
    else:
        semantic_cache.store_response(query, response.content, total_tokens)
        stats.cached_responses += 1
        print("üíæ Response cached")

    return response.content, reranked_docs, selected_files

def retrieve_summary_ids(query, k=30):
    docs = hybrid_summary_search(query, top_k=k)
    return [doc.metadata["chunk_id"] for doc in docs]

def evaluate_retriever(eval_dataset, k_values=[5, 10, 20]):
    all_results = []

    for item in eval_dataset:
        query = item["query"]
        relevant = item["relevant_chunks"]

        retrieved_ids = retrieve_summary_ids(query, k=max(k_values))

        metrics = {
            "query": query,
            "MRR": mrr(retrieved_ids, relevant),
        }

        for k in k_values:
            metrics[f"Precision@{k}"] = precision_at_k(retrieved_ids, relevant, k)
            metrics[f"Recall@{k}"] = recall_at_k(retrieved_ids, relevant, k)
            metrics[f"nDCG@{k}"] = ndcg_at_k(retrieved_ids, relevant, k)

        all_results.append(metrics)

    return all_results

eval_dataset = [
    {"query": "Your Query?", "relevant_chunks": [1]},
    {"query": "Your Query?", "relevant_chunks": [5,6,7,8,9,10]},
    {"query": "Your Query?", "relevant_chunks": [11]},
    {"query": "Your Query", "relevant_chunks": [9, 42, 43, 44, 45, 46, 48]},
]


# --------------------------------------------------------------
# 10. RUN QUERY & METRICS
# --------------------------------------------------------------
query = ("Your Query???")

start = time.perf_counter()
response, docs, selected_files = answer_query(query)
elapsed = time.perf_counter() - start

print("\n" + "=" * 50)
print("SELECTED FILES:", docs)
print("=" * 50)
print("\n========== ANSWER ==========")
print(response)
print(f"\nElapsed time: {elapsed:.2f} sec")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
print("\n--- STATS ---")
print(f"LLM calls: {stats.llm_calls}")
print(f"Cache hits: {stats.cache_hits}")
print(f"Spent tokens: {stats.spent_tokens}")
print(f"Saved tokens: {stats.saved_tokens}")

metrics = evaluate_retriever(eval_dataset)

print("\nüìä RETRIEVAL EVALUATION RESULTS\n")
for r in metrics:
    print("\nQuery:", r["query"])
    print("MRR:", r["MRR"])
    for k in [5, 10, 20]:
        print(f"Precision@{k}: {r[f'Precision@{k}']}")
        print(f"Recall@{k}:    {r[f'Recall@{k}']}")
        print(f"nDCG@{k}:      {r[f'nDCG@{k}']}")
